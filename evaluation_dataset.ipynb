{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_directory = os.path.abspath('') + \"/repository\"\n",
    "models_jsons = os.listdir(repository_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_model_dict = {}\n",
    "\n",
    "for model_file in models_jsons:\n",
    "    with open(repository_directory + \"/\" + model_file) as model_json:\n",
    "        data = json.load(model_json)\n",
    "        for dataset in data['dataset']:\n",
    "            if dataset not in dataset_model_dict:\n",
    "                dataset_model_dict[dataset] = []\n",
    "            \n",
    "            dataset_model_dict[dataset].append(data['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'squad_v2': ['mrm8488/longformer-base-4096-finetuned-squadv2',\n",
       "  'allenai/unifiedqa-t5-base',\n",
       "  'ixa-ehu/SciBERT-SQuAD-QuAC'],\n",
       " 'hotpot_qa': ['AdapterHub/roberta-base-pf-hotpotqa'],\n",
       " 'cuad': ['Rakib/roberta-base-on-cuad', 'akdeniz27/deberta-v2-xlarge-cuad'],\n",
       " 'trivia_qa': ['allenai/longformer-large-4096-finetuned-triviaqa'],\n",
       " 'squad': ['ozcangundes/T5-base-for-BioQA',\n",
       "  'MaRiOrOsSi/t5-base-finetuned-question-answering',\n",
       "  'vanadhi/roberta-base-fiqa-flm-sq-flit'],\n",
       " 'BeIR/bioasq-generated-queries': ['ozcangundes/T5-base-for-BioQA'],\n",
       " 'duorc': ['MaRiOrOsSi/t5-base-finetuned-question-answering',\n",
       "  'MaRiOrOsSi/t5-base-finetuned-question-answering'],\n",
       " 'pubmed_qa': ['razent/SciFive-base-Pubmed_PMC', 'microsoft/biogpt'],\n",
       " 'zhengyun21/PMC-Patients': ['razent/SciFive-base-Pubmed_PMC'],\n",
       " 'boolq': ['allenai/unifiedqa-t5-base'],\n",
       " 'race': ['allenai/unifiedqa-t5-base'],\n",
       " 'quoref': ['allenai/unifiedqa-t5-base'],\n",
       " 'ropes': ['allenai/unifiedqa-t5-base'],\n",
       " 'drop': ['allenai/unifiedqa-t5-base'],\n",
       " 'sagnikrayc/mctest': ['allenai/unifiedqa-t5-base'],\n",
       " 'qasc': ['allenai/unifiedqa-t5-base'],\n",
       " 'math_qa': ['AlexWortega/taskGPT2-xl-v0.2a'],\n",
       " 'gsm8k': ['AlexWortega/taskGPT2-xl-v0.2a'],\n",
       " 'covid_qa_deepset': ['Sarmila/pubmed-bert-squad-covidqa'],\n",
       " 'quac': ['ixa-ehu/SciBERT-SQuAD-QuAC']}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rows_from_dataset(dataset: str,\n",
    "                             column_names: tuple,\n",
    "                             *args,\n",
    "                             num_samples: int = 250,\n",
    "                             seed: int = 42,\n",
    "                             **kwargs) -> pd.DataFrame:    \n",
    "    if not isinstance(column_names, tuple):\n",
    "        raise Exception(\"Column names need to be a list of column names as strings.\")\n",
    "    try:\n",
    "        dataset = load_dataset(dataset, *args, split=\"test\")\n",
    "    except Exception as e:\n",
    "        print(\"Could NOT load dataset for {0}\".format(dataset))\n",
    "        raise Exception(\"Error while loading dataset {}\".format(e))\n",
    "    shuffled_dataset = dataset.shuffle(seed=seed)\n",
    "    df = pd.DataFrame(shuffled_dataset[:num_samples])\n",
    "    try:\n",
    "        return df[list(column_names)]\n",
    "    except KeyError as e:\n",
    "        raise e\n",
    "    \n",
    "    \n",
    "def sample_rows_from_dataset(dataset: str,\n",
    "                             column_names: tuple,\n",
    "                             *args,\n",
    "                             num_samples: int = 3000,\n",
    "                             seed: int = 42,\n",
    "                             **kwargs) -> pd.DataFrame:\n",
    "    if not isinstance(column_names, tuple):\n",
    "        raise Exception(\"Column names need to be a list of column names as strings.\")\n",
    "    try:\n",
    "        dataset = load_dataset(dataset, *args, **kwargs)\n",
    "    except Exception as e:\n",
    "        print(\"Could NOT load dataset for {0}\".format(dataset))\n",
    "        raise Exception(\"Error while loading dataset {}\".format(e))\n",
    "    shuffled_dataset = dataset.shuffle(seed=seed)\n",
    "    df = pd.DataFrame(shuffled_dataset[:num_samples])\n",
    "    try:\n",
    "        return df[list(column_names)]\n",
    "    except KeyError as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"squad\"\n",
    "configs = None\n",
    "column_tuple = (\"question\", \"context\", \"answers\")\n",
    "\n",
    "squad_qa_dataset = sample_rows_from_dataset(dataset_name, column_tuple, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for i in range(len(squad_qa_dataset)):\n",
    "    curr_ans_list = squad_qa_dataset['answers'][i]['text']\n",
    "    curr_ans = max(curr_ans_list, key = len)\n",
    "    answers.append(curr_ans)\n",
    "    \n",
    "squad_qa_dataset['answers'] = answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pubmed Biology Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"pubmed_qa\"\n",
    "config = \"pqa_labeled\"\n",
    "column_tuple = (\"question\", \"context\", \"long_answer\")\n",
    "\n",
    "pubmed_qa_dataset = sample_rows_from_dataset(dataset_name, column_tuple, config, split=\"train\")\n",
    "\n",
    "contexts_strings = []\n",
    "\n",
    "for i in range(len(pubmed_qa_dataset)):\n",
    "    contexts_strings.append(' '.join(pubmed_qa_dataset[\"context\"][i]['contexts']))\n",
    "    \n",
    "pubmed_qa_dataset['context'] = contexts_strings\n",
    "pubmed_qa_dataset = pubmed_qa_dataset.rename(columns={\"long_answer\": \"answers\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BioASQ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 14.0k/14.0k [00:00<00:00, 40.1MB/s]\n",
      "Downloading data: 100%|██████████| 7.12G/7.12G [09:22<00:00, 12.7MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [09:22<00:00, 562.45s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [01:16<00:00, 76.97s/it]\n",
      "Generating train split: 14100000 examples [01:29, 158196.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"BeIR/bioasq-generated-queries\"\n",
    "column_tuple = (\"text\", \"query\")\n",
    "\n",
    "bioasq_qa_dataset = sample_rows_from_dataset(dataset_name, column_tuple, split=\"train\")\n",
    "bioasq_qa_dataset = bioasq_qa_dataset.rename(columns={\"text\": \"context\", \"query\": \"question\"})\n",
    "bioasq_qa_dataset = bioasq_qa_dataset[[\"question\", \"context\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuad (legal) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"cuad\"\n",
    "column_tuple = (\"question\", \"context\", \"answers\")\n",
    "\n",
    "cuad_qa_dataset = sample_rows_from_dataset(dataset_name, column_tuple, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for i in range(len(cuad_qa_dataset)):\n",
    "    curr_ans_list = cuad_qa_dataset['answers'][i]['text']\n",
    "    if len(curr_ans_list)!=0:\n",
    "        curr_ans = max(curr_ans_list, key = len)\n",
    "    else:\n",
    "        curr_ans = \"\"\n",
    "    answers.append(curr_ans)\n",
    "    \n",
    "cuad_qa_dataset['answers'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuad_qa_dataset = cuad_qa_dataset[cuad_qa_dataset[\"answers\"]!=\"\"][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuad_qa_dataset[\"domain\"] = \"legal\"\n",
    "#bioasq_qa_dataset[\"domain\"] = \"bio\"\n",
    "pubmed_qa_dataset[\"domain\"] = \"bio\"\n",
    "squad_qa_dataset[\"domain\"] = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['razent/SciFive-base-Pubmed_PMC', 'microsoft/biogpt']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_model_dict['pubmed_qa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuad_qa_dataset['models'] = cuad_qa_dataset['models'].apply(lambda x: dataset_model_dict['cuad'])\n",
    "pubmed_qa_dataset['models'] = pubmed_qa_dataset['models'].apply(lambda x: dataset_model_dict['pubmed_qa'])\n",
    "squad_qa_dataset['models'] = squad_qa_dataset['models'].apply(lambda x: dataset_model_dict['squad'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = pd.concat([cuad_qa_dataset, pubmed_qa_dataset, squad_qa_dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answers</th>\n",
       "      <th>domain</th>\n",
       "      <th>models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>What increases student's motivation to learn?</td>\n",
       "      <td>Teachers that exhibit enthusiasm can lead to s...</td>\n",
       "      <td>teacher enthusiasm</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>In 1984 Thomas Murphy contacted Leonard Golden...</td>\n",
       "      <td>In December 1984, Thomas S. Murphy, chief exec...</td>\n",
       "      <td>Capital Cities Communications</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>How many chloroplasts are in stomatal guard ce...</td>\n",
       "      <td>In some plants such as cacti, chloroplasts are...</td>\n",
       "      <td>8–15 per cell</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>What physical quantities do not have direction?</td>\n",
       "      <td>Forces act in a particular direction and have ...</td>\n",
       "      <td>denoted scalar quantities</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>Who appoints elders?</td>\n",
       "      <td>Elders are called by God, affirmed by the chur...</td>\n",
       "      <td>the local church</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>What organization is the IPCC a part of?</td>\n",
       "      <td>The Intergovernmental Panel on Climate Change ...</td>\n",
       "      <td>the United Nations</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>What church in Virginia is maintained by Hugue...</td>\n",
       "      <td>Paul Revere was descended from Huguenot refuge...</td>\n",
       "      <td>Manakin Episcopal Church</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>Where is the oldest pharmacy stated to be loca...</td>\n",
       "      <td>In Europe there are old pharmacies still opera...</td>\n",
       "      <td>Church of Santa Maria Novella in Florence, Italy</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>What are the three primary expressions used to...</td>\n",
       "      <td>The best, worst and average case complexity re...</td>\n",
       "      <td>best, worst and average case complexity</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>What are those with lower incomes often unable...</td>\n",
       "      <td>Firstly, certain costs are difficult to avoid ...</td>\n",
       "      <td>their finances</td>\n",
       "      <td>None</td>\n",
       "      <td>[ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  \\\n",
       "740      What increases student's motivation to learn?   \n",
       "741  In 1984 Thomas Murphy contacted Leonard Golden...   \n",
       "742  How many chloroplasts are in stomatal guard ce...   \n",
       "743    What physical quantities do not have direction?   \n",
       "744                               Who appoints elders?   \n",
       "745           What organization is the IPCC a part of?   \n",
       "746  What church in Virginia is maintained by Hugue...   \n",
       "747  Where is the oldest pharmacy stated to be loca...   \n",
       "748  What are the three primary expressions used to...   \n",
       "749  What are those with lower incomes often unable...   \n",
       "\n",
       "                                               context  \\\n",
       "740  Teachers that exhibit enthusiasm can lead to s...   \n",
       "741  In December 1984, Thomas S. Murphy, chief exec...   \n",
       "742  In some plants such as cacti, chloroplasts are...   \n",
       "743  Forces act in a particular direction and have ...   \n",
       "744  Elders are called by God, affirmed by the chur...   \n",
       "745  The Intergovernmental Panel on Climate Change ...   \n",
       "746  Paul Revere was descended from Huguenot refuge...   \n",
       "747  In Europe there are old pharmacies still opera...   \n",
       "748  The best, worst and average case complexity re...   \n",
       "749  Firstly, certain costs are difficult to avoid ...   \n",
       "\n",
       "                                              answers domain  \\\n",
       "740                                teacher enthusiasm   None   \n",
       "741                     Capital Cities Communications   None   \n",
       "742                                     8–15 per cell   None   \n",
       "743                         denoted scalar quantities   None   \n",
       "744                                  the local church   None   \n",
       "745                                the United Nations   None   \n",
       "746                          Manakin Episcopal Church   None   \n",
       "747  Church of Santa Maria Novella in Florence, Italy   None   \n",
       "748           best, worst and average case complexity   None   \n",
       "749                                    their finances   None   \n",
       "\n",
       "                                                models  \n",
       "740  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "741  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "742  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "743  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "744  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "745  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "746  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "747  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "748  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  \n",
       "749  [ozcangundes/T5-base-for-BioQA, MaRiOrOsSi/t5-...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.to_csv(\"eval_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11-632",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
